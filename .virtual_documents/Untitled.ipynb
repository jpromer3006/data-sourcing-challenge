





import requests
import time
from dotenv import load_dotenv
import os
import pandas as pd
import json
# Set environment variables from the .env in the local environment
load_dotenv()
api_key = os.getenv("NYT_API_KEY")
print(api_key)
# Set the base URL
url = "https://api.nytimes.com/svc/search/v2/articlesearch.json?"
# Filter for movie reviews with "love" in the headline
# section_name should be "Movies"
# type_of_material should be "Review"
# field-name-1:("value1") AND field-name-2:("value2", "value3")
filter_query = 'section_name:"Movies" AND type_of_material:"Review" AND headline:"love"'
# Use a sort filter, sort by newest
sorted_filter="newest"
# Select the following fields to return:
# headline, web_url, snippet, source, keywords, pub_date, byline, word_count
field_list = "headline,web_url,snippet,source,keywords,pub_date,byline,word_count"
begin_date = "20130101"
end_date = "20230531"
query_url = f"{url}api-key={api_key}&begin_date={begin_date}&end_date={end_date}" + f'&fq={filter_query}&sort={sorted_filter}&fl={field_list}'
# cal the api
reviews_list = []
for page in range(0,20):
    query_url = f"{query_url}&page={page}"
    reviews = requests.get(query_url).json()
    time.sleep(12)
    try:
        # loop through the reviews["response"]["docs"] and append each review to the list
        for review in reviews["response"]["docs"]:
            reviews_list.append(review)
        # Print the page that was just retrieved
        print(f"Checked page {page}")
    except:
        # Print the page number that had no results then break from the loop
        print(f"No results. Ended at page {page}.")
        break



query_url


# Preview the first 5 results in JSON format
# Use json.dumps with argument indent=4 to format data
print(json.dumps(reviews_list[:5], indent=4))



